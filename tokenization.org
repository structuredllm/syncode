#+title: Non-homomorphic tokenization in existing tokenizers.
#+author: Preston Miller Firestone
#+date: <2025-02-02 Sun>
#+macro: color @@html:<span style="color: $1">$2</span>@@
#+property: header-args:python :session :exports both
#+begin_src elisp :exports none
  (setq-local org-confirm-babel-evaluate nil)
#+end_src

#+begin_src python :session :exports none
  from transformers import AutoTokenizer
  tokenizer = AutoTokenizer.from_pretrained('gpt2')
#+end_src

#+RESULTS:
: None

#+RESULTS:

Computers deal with numbers, but humans deal with text. The tokenizer is the component of an LLM that converts between the numbers the model interacts with and the text the humans interact with. This blog post explains a peculiar corner of some really-existing tokenizers.

* Characters, graphemes, codepoints, encodings, bytes, oh my!
This section introduces code point​s, character​s, and character encoding scheme​s. I include the relevant concepts from the Unicode Standard's glossary [[cite:&commitee25_unicod_stand]] in a footnote.[fn:1] I will attempt to use these terms assiduously as they are defined in the Unicode Standard, even though these meanings are not always intuitive. An abstract character is a unit of information used for the storage and manipulation of text. A code point is a number. An encoded character is a mapping between an abstract character and a code point. I will represent code point​s as hexidecimal numbers preceded by "U+".

#+begin_quote
你 is an abstract character, pronounced /nǐ/ and meaning "you".

U+4F60 is a code point, a number whose decimal value is 20320.

The mapping 你 ↔ U+4F60 is an encoded character, indicating that under the Unicode Standard, the abstract character 你 is uniquely associated with the code point U+4F60.
#+end_quote

We can represent these code point​s in the computer in many different ways. A system of mapping from code point​s to a binary representation in memory is called a character encoding scheme. For a number of practical and historical reasons, the dominant character encoding scheme is UTF-8,[fn:2] which, as of 2025, is used by 98.5% of all websites.[fn:3]

Here is a worked example:
#+begin_quote
Abstract character sequence: {{{color(red, 你)}}}{{{color(green, 好)}}}{{{color(blue, 吗)}}}？

Code point​s: {{{color(red, U+4F60)}}} {{{color(green, U+597D)}}} {{{color(blue, U+5417)}}} U+FF1F

UTF-8: {{{color(red, E4 BD A0)}}} {{{color(green, E5 A5 BD)}}} {{{color(blue, E5 90 97)}}} EF BC 9F
#+end_quote

In the computer, the bytes represented on the last row will be stored in the file. When the user opens the file, the program used will recognize the encoding as UTF-8, map the bytes to the code point​s, then render the code point​s using the user's selected font.

* Tokenization[fn:4]
Humans interact with text, which is built out of characters. As we have seen above, the internal representation of these characters is not trivial or obvious, but the layers of abstraction used to display text to the user and to take it in from them are well understood. Transformer-based language models, on the other hand, interact with embedding vectors. Typically these are encapsulated as input ids, or token indices.[fn:5] An input id is an integer drawn from a bounded range, usually from about 30k to 120k unique integers. These integers index a lookup table of embedding vectors, one per input id.

The tokenizer is the component that maps between text the user sees and input ids the model sees. Tokenization is the process of converting text into input ids, and detokenization is the inverse conversion from input ids to text.[fn:6] In principle, this operation is homomorphic, which [[cite:&geng2024bytebpetokenizationinverse]] define as follows:
#+begin_quote
Given two operations ⊕ and ⊙ on two alphabets Σ∗ and N∗ respectively, a function h : Σ∗ → N∗ is a string homomorphism if ∀u, v ∈ Σ∗, h(u ⊕ v) = h(u) ⊙ h(v).
#+end_quote
Consider ⊕ to be string concatenation and ⊙ to be the concatenation of sequences of integers. This means, intuitively, that cutting a string in two, tokenizing its two parts, and appending the resulting lists of input ids will always get you the same thing you would have gotten if you had tokenized the string all at once. Similarly, the inverse homomorphism holds that if you detokenize two lists of input ids and concatenate the resulting string, then the string you get will be the same as if you had concatenated the lists of input ids and detokenized.

To clarify this, let us take some examples. Here we're using GPT-2's tokenizer [[cite:&radford19_languag_model_unsup_multit_learn]]. Let's begin by looking at the input ids you get from this string.

#+begin_export markdown
```python
>>> tokenizer.encode("Hello, tokenizing world!")
[15496, 11, 11241, 2890, 995, 0]
```
#+end_export

And if we detokenize, we get back the string we started with.

#+begin_export markdown
```python
>>> tokenizer.decode([15496, 11, 11241, 2890, 995, 0])
"Hello, tokenizing world!"
```
#+end_export


If we encode two parts of the string seperately, we can get back a different list of input ids.
#+begin_export markdown
```python
>>> tokenizer.encode("Hello, tokeniz") + tokenizer.encode("ing world!")
[15496, 11, 11241, 528, 278, 995, 0]
```
#+end_export
This means that tokenization, in this implementation, is not homomorphic: we just showed an example where the property was violated, to wit,
$$
tokenize(\textrm{"Hello, tokeniz"} ⊕ \textrm{"ing world!"}) \ne tokenize(\textrm{"Hello, tokeniz"}) ⊙ tokenize(\textrm{"ing world!"})
$$

[[cite:&geng2024bytebpetokenizationinverse]] show by a similar example that tokenization is not homomorphic, but that the detokenization procedure is homomorphic.

#+begin_export markdown
```python
>>> tokenizer.decode([15496, 11, 11241] + [528, 278, 995, 0])
"Hello, tokenizing world!"
```
#+end_export
#+begin_export markdown
```python
>>> tokenizer.decode([15496, 11, 11241]) + tokenizer.decode([528, 278, 995, 0])
"Hello, tokenizing world!"
```
#+end_export
We get the whole string back. This example satisfies our homomorphism property described above, since concatenating strings and concatenating lists of input ids are equivalent.

$$
detokenize([15496, 11, 11241] ⊙ [528, 278, 995, 0]) = detokenize([15496, 11, 11241]) ⊕ detokenize([528, 278, 995, 0])
$$

Let's try a naughtier example to see whether detokenization is always homomorphic for this tokenizer.

#+begin_export markdown
```python
>>> tokenizer.encode('∀')
[24861, 222]
```
#+end_export

We're already getting into strange territory: this is a single character, but we get two input ids. This seems counter-intuitive, since we'd expect an input id to be at least one character big. When we detokenize the pair we get back what we expect.

#+begin_export markdown
```python
>>> tokenizer.decode([24861, 222])
"∀"
```
#+end_export

But if we attempt to detokenize the individual input ids...

#+begin_export markdown
```python
>>> tokenizer.decode([24861])
"�"
```
#+end_export
we get nonsense results...
#+begin_export markdown
```python
>>> tokenizer.decode([222])
"�"
```
#+end_export
that don't behave the way we want them to.
#+begin_export markdown
```python
>>> tokenizer.decode([24861]) + tokenizer.decode([222])
"��"
```
#+end_export

What's going wrong? Why does this particular example break the homomorphism of detokenization? Are there other examples that behave a similar way? To answer this we'll have to go deeper into what's going on under the hood.

* Everything you've been told is a lie
So far I have elided the conversion from character sequences to input ids by saying that the tokenizer maps from one to the other. This isn't quite true: in practice, many tokenizers break the text into chunks of characters, then turn those chunks into input ids. Those chunks are tokens properly speaking, and they're what's learned by Byte Pair Encoding.

Let's revisit our devilish ∀ example, using some APIs from Huggingface we haven't had the opportunity to use yet: in addition to mapping between characters and input ids, we can map between input ids and the characters they represent.
#+begin_export markdown
```python
>> tokenizer.convert_ids_to_tokens([24861])
['âĪ']
```
#+end_export
#+begin_export markdown
```python
>>> tokenizer.onvert_ids_to_tokens([222])
['Ģ']
```
#+end_export

#+begin_export markdown
```python
>>> tokenizer.convert_ids_to_tokens([24861, 222])
['âĪ', 'Ģ']
```
#+end_export

#+begin_export markdown
```python
>>> tokenizer.convert_ids_to_tokens([24861])+ tokenizer.convert_ids_to_tokens([222])
['âĪ', 'Ģ']
```
#+end_export

What happens if we try to convert these tokens to strings?
#+begin_export markdown
```python
>>> tokenizer.convert_tokens_to_string(['âĪ'])
"�"
```
#+end_export
#+begin_export markdown
```python
>>> tokenizer.convert_tokens_to_string(['Ģ'])
"�"
```
#+end_export

We get back the same nonsense characters we had before, with no ∀ in sight. This is exceedingly bizarre. Where are these strange characters coming from? Astonishingly, the tokenizer is able to reconstruct the character from the concatenated tokens...

#+begin_export markdown
```python
>>> tokenizer.convert_tokens_to_string(['âĪ'] + ['Ģ'])
"∀"
```
#+end_export

even if we cut the tokens apart into single characters.
#+begin_export markdown
```python
>>> tokenizer.convert_tokens_to_string(['â'] + ['Ī'] + ['Ģ'])
"∀"
```
#+end_export

Where is this strange behavior coming from? We've chased it down to these weird mappings between input ids, tokens, and strings, but where doe these odd characters that make up the tokens come from?

* Bytes to Code Point​s
The ultimate explanation is found in the following code, which comes from the GPT-2 repository.[fn:7] A Rust translation appears HuggingFace's tokenizer library.[fn:8]
#+begin_export markdown
```python
def bytes_to_unicode():
  """
  Returns list of utf-8 byte and a corresponding list of unicode strings.
  The reversible bpe codes work on unicode strings.
  This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.
  When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.
  To avoid that, we want lookup tables between utf-8 bytes and unicode strings.
  And avoids mapping to whitespace/control characters the bpe code barfs on.
  """
  bs = list(range(ord("!"), ord("~")+1))+list(range(ord("¡"), ord("¬")+1))+list(range(ord("®"), ord("ÿ")+1))
  cs = bs[:]
  n = 0
  for b in range(2**8):
      if b not in bs:
          bs.append(b)
          cs.append(2**8+n)
          n += 1
  cs = [chr(n) for n in cs]
  return dict(zip(bs, cs))
```
#+end_export
The GPT-2 paper does not mention this [[cite:&radford19_languag_model_unsup_multit_learn]], nor are the commit messages that add the code to GPT-2 or tokenizers very informative. A form of this code is included in tiktoken to provide legacy support for GPT-2.[fn:9] As far as I can tell, none of the other tokenizers for newer OpenAI models have this behavior. However, several models still display this behavior: the Codegen series, the Llama series, and DeepSeek AI's models (including DeepSeek-R1) all act this way. This behavior is documented in tokenizer's repository.[fn:10][fn:11]

There are two questions to answer at this point: why do we do this, and what does this do? It is easier to begin by answering the "what" question; once we know what is happening we will be able to explain why we are doing it by referencing what the result of this transformation is.

Simply, this is a one-to-one map from byte values to unicode code points. This is a devilish hack that makes many of the tokens in the vocabulary look like random noise and is the source of the strange behavior we observed in the previous section. When the tokenizer receives a series of bytes in UTF-8, it passes each byte through this dictionary. The bytes that represent visible characters of ASCII, 21_{16} through 7E_{16}, are mapped to themselves. The other bytes, both those that represent invisible ASCII characters (whitespace and control characters) are mapped to other code point​s in the Unicode codespace.

For readability, I define the forward and backward dictionaries like so:
#+begin_export markdown
```python
byte_dict = bytes_to_unicode()
dict_byte = {v: k for k, v in byte_dict.items()} # Inverse mapping.
```
#+end_export

Now we can begin to explore the case we examined above. Let's begin by getting the code point representing each of the bytes in the UTF-8 encoding of ∀.
#+begin_export markdown
```python
>>> [byte_dict[byte] for byte in '∀'.encode()]
['â', 'Ī', 'Ģ']
```
#+end_export
We can confirm by passing these characters through the inverse mapping and representing them as hexadecimal bytes.
#+begin_export markdown
```python
>>> [bytes([dict_byte[char]]) for char in ['â', 'Ī', 'Ģ']]
[b'\xe2', b'\x88', b'\x80']
```
#+end_export

This is exactly the three bytes of the UTF-8 encoding of ∀:
#+begin_export markdown
```python
>>> '∀'.encode()
b'\xe2\x88\x80'
```
#+end_export

This trick turns each byte of the input into the corresponding code point. That way we can represent the input as Unicode code points and work with it as a string in the space of the character abstraction. We can learn the byte pair encodings beginning with a 256-member vocabulary, since we have one for each byte.

* BPE
Byte pair encoding is a compression algorithm. It finds most-frequently appearing pairs of adjacent bytes in the input data with a byte that was not in the original data. Along with the compressed data, the algorithm writes out a table of pair substitutions [[cite:&gage94_new_algor_data_compr]].

[[cite:&sennrich-etal-2016-neural]] introduced byte pair encoding to natural language processing as a way to represent an open vocabulary of a language through a fixed-size vocabulary of character sequences, avoiding out-of-vocabulary errors while efficiently representing the input text. [[cite:&Berglund_2023]] provides a formal analysis of the algorithm and the problem it solves. The current fastest implementation of the algorithm scales linearly in the length of its input [[cite:&antwerpen24_so]].

* Bibliography
[[bibliography:/home/pmf/src/syncode-trees/rust/tokenization.bib]]

* Footnotes

[fn:1]
#+begin_quote
/<<<Abstract Character>>>./ A unit of information used for the organization, control, or representation of textual data.

/<<<Character>>>/. (1) The smallest component of written language that has semantic value; refers to the abstract meaning and/or shape, rather than a specific shape (see also glyph), though in code tables some form of visual representation is essential for the reader’s understanding. (2) Synonym for abstract character. (3) The basic unit of encoding for the Unicode character encoding.

/<<<Character Encoding Form>>>./ Mapping from a character set definition to the actual code units used to represent the data.

/<<<Character Encoding Scheme>>>./ A character encoding form plus byte serialization. There are seven character encoding schemes in Unicode: UTF-8, UTF-16, UTF-16BE, UTF-16LE, UTF-32, UTF-32BE, and UTF-32LE.

/<<<Character Set>>>./ A collection of elements used to represent textual information.

/<<<Code Point>>>./ (1) Any value in the Unicode codespace; that is, the range of integers from 0 to 10FFFF16. (See definition D10 in Section 3.4, Characters and Encoding.) Not all code points are assigned to encoded characters. See code point type. (2) A value, or position, for a character, in any coded character set.

/<<<Encoded character>>>/. An association (or mapping) between an abstract character and a code point. 

/<<<Glyph>>>./ (1) An abstract form that represents one or more glyph images. (2) A synonym for glyph image. In displaying Unicode character data, one or more glyphs may be selected to depict a particular character. These glyphs are selected by a rendering engine during composition and layout processing.

/<<<Glyph Image>>>./ The actual, concrete image of a glyph representation having been rasterized or otherwise imaged onto some display surface.

/<<<Grapheme>>>./ (1) A minimally distinctive unit of writing in the context of a particular writing system. For example, ‹b› and ‹d› are distinct graphemes in English writing systems because there exist distinct words like big and dig. Conversely, a lowercase italiform letter a and a lowercase Roman letter a are not distinct graphemes because no word is distinguished on the basis of these two different forms. (2) What a user thinks of as a character.
#+end_quote
https://www.unicode.org/glossary/index.html

[fn:2]The details of how UTF-8 encodings are computed for a given code point are not significant to this blog post. The interested reader is directed to [[cite:&commitee25_unicod_stand 3.9.3]] for details and [[cite:&pike93_hello_world_καλημ]] for an early account of the encoding scheme. As always, the [[https://en.wikipedia.org/wiki/UTF-8][relevant Wikipedia page]] is also excellent.

[fn:3]https://w3techs.com/technologies/cross/character_encoding/ranking

[fn:4]This discussion is based on [[cite:&geng2024bytebpetokenizationinverse]] and [[cite:&cognetta2024tokenizationfinitestatetransduction]].

[fn:5]This is huggingface's terminology. See https://huggingface.co/docs/transformers/glossary#input-ids.

[fn:6]Confusingly, the huggingface API exposes these procedures as src_python[:exports code]{tokenizer.encode} and src_python[:exports code]{tokenizer.decode} respectively. To prevent name collisions with the concepts in Unicode, I will refer to these as tokenization and detokenization within the text.

[fn:7]https://github.com/openai/gpt-2/blob/9b63575ef42771a015060c964af2c3da4cf7c8ab/src/encoder.py#L9

[fn:8]https://github.com/huggingface/tokenizers/blob/c45aebd1029acfbe9e5dfe64e8b8441d9fae727a/tokenizers/src/pre_tokenizers/byte_level.rs#L14

[fn:9]https://github.com/openai/tiktoken/blob/63527649963def8c759b0f91f2eb69a40934e468/tiktoken/load.py#L84

[fn:10]https://github.com/huggingface/tokenizers/blob/c45aebd1029acfbe9e5dfe64e8b8441d9fae727a/docs/source/components.rst

[fn:11]The bpe crate released by GitHub works with pre-trained vocabulary lists; it does not use merges and cannot train a new byte pair encoding from a corpus: it relies on existing vocabulary lists [[cite:&antwerpen24_so]]. It also works directly on the underlying bytes, unlike the BPE implementation used here. Therefore it does not show this behavior.
p
[fn:12]The form of cultural imperialism that makes ASCII the default encoding for most computer tools.
